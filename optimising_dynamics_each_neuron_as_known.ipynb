{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rna_data = pd.read_csv(\"gene_data/rna_common_complete.csv\")\n",
    "rna_data = rna_data.sort_values(by=['sn','period']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_og_shape = rna_data.drop(['sn','group','caarms_status','period'],axis=1).values\n",
    "X_reshaped = X_og_shape.reshape(len(set(rna_data['sn'])), 3, X_og_shape.shape[1])\n",
    "labels_group = rna_data[rna_data['period'] == 24]['group'].values\n",
    "labels = [0 if i == 'C' else 1 for i in labels_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from neucube.utils import SNR\n",
    "from neucube.utils import interpolate\n",
    "from neucube.encoder import Delta\n",
    "\n",
    "ratios = SNR(X_reshaped[:,0,:], labels)\n",
    "top_idx = torch.argsort(ratios, descending=True)[0:20]\n",
    "X_reshaped_topidx = X_reshaped[:,:,top_idx]\n",
    "interpolated_X = interpolate(X_reshaped_topidx, num_points=104)\n",
    "\n",
    "encoder = Delta(threshold=0.008)\n",
    "X = encoder.encode_dataset(interpolated_X)\n",
    "y = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neucube.sampler import TemporalBinning\n",
    "from neucube.utils import SeparationIndex\n",
    "neuron_parm_dict = { \n",
    "    'rs' : {'a': 0.02, 'b': 0.2, 'c': -65, 'd': 8}, \n",
    "    'ch' : {'a': 0.02, 'b': 0.55, 'c': -45, 'd': 4},\n",
    "    'ib' : {'a': 0.06, 'b': 0.55, 'c': -55, 'd': 3},\n",
    "}\n",
    "\n",
    "def objective_function(res_, X_stimuli, labels, params):\n",
    "    a, b, c, d = [torch.tensor(list(map(lambda x: neuron_parm_dict[x][i], params[0]))) for i in ['a', 'b', 'c', 'd']]\n",
    "    res_.update_parms(a=a, b=b, c=c, d=d)\n",
    "    out_spikes = res_.simulate(X_stimuli, mem_thr=30, train=False, verbose=True)\n",
    "    sampler = TemporalBinning(bin_size=10)\n",
    "    state_vec = sampler.sample(out_spikes)\n",
    "    return SeparationIndex(state_vec, labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neucube import IzhReservoir\n",
    "izh_res = IzhReservoir(inputs=X.shape[2], c=0.7, l=0.18, input_conn_prob=0.85)\n",
    "init_n_type = np.random.choice(['rs','ch','ib'], izh_res.n_neurons, replace=True)\n",
    "izh_res.set_exc_parms(a=0.06, b=0.55, c=-55, d=3)\n",
    "izh_res.set_inh_parms(a=0.01, b=0.2, c=-65, d=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = [torch.tensor(list(map(lambda x: neuron_parm_dict[x][i], init_n_type))) for i in ['a', 'b', 'c', 'd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal neuron types: (('ch', 'ch', 'ib', 'ib', 'ch', 'ib', 'ib', 'rs', 'ib', 'ch', 'ch', 'ib', 'rs', 'ch', 'rs', 'ib', 'ib', 'ch', 'rs', 'ib', 'ib', 'rs', 'ch', 'ch', 'rs', 'ib', 'ch', 'rs', 'ib', 'rs', 'rs', 'rs', 'ib', 'ch', 'ib', 'ch', 'ch', 'rs', 'rs', 'ib', 'ch', 'ch', 'rs', 'rs', 'ch', 'ib', 'ch', 'rs', 'ib', 'ch', 'ch', 'ib', 'rs', 'ib', 'rs', 'rs', 'ib', 'ib', 'ib', 'ch', 'ib', 'rs', 'rs', 'ib', 'ib', 'ib', 'rs', 'ch', 'ib', 'rs', 'rs', 'rs', 'ch', 'ch', 'ch', 'rs', 'ib', 'rs', 'ch', 'ib', 'rs', 'ch', 'ch', 'rs', 'ch', 'rs', 'ib', 'ib', 'ch', 'ch', 'ch', 'rs', 'ib', 'rs', 'ib', 'rs', 'rs', 'rs', 'rs', 'ib', 'rs', 'ch', 'rs', 'ib', 'ib', 'ch', 'ch', 'ib', 'ch', 'ib', 'ib', 'rs', 'ib', 'ch', 'rs', 'rs', 'rs', 'rs', 'ib', 'ib', 'rs', 'ib', 'rs', 'ib', 'rs', 'ch', 'ch', 'ch', 'ib', 'ib', 'ch', 'ib', 'rs', 'rs', 'ib', 'rs', 'rs', 'ib', 'rs', 'ib', 'ib', 'ch', 'ib', 'ch', 'ib', 'rs', 'ib', 'ib', 'rs', 'rs', 'rs', 'rs', 'ch', 'ch', 'ch', 'rs', 'ib', 'rs', 'ib', 'rs', 'ib', 'rs', 'ch', 'ib', 'ch', 'ch', 'rs', 'ib', 'rs', 'ib', 'ch', 'rs', 'ch', 'ch', 'ib', 'ib', 'rs', 'ib', 'rs', 'rs', 'ch', 'ch', 'ch', 'rs', 'ch', 'rs', 'ib', 'rs', 'ch', 'rs', 'ib', 'rs', 'rs', 'ch', 'rs', 'ch', 'rs', 'ch', 'ib', 'ch', 'rs', 'rs', 'ch', 'ch', 'ch', 'rs', 'rs', 'ch', 'rs', 'ch', 'rs', 'rs', 'ib', 'ib', 'ch', 'ch', 'ch', 'ib', 'rs', 'ib', 'rs', 'ib', 'ib', 'rs', 'ch', 'rs', 'ib', 'rs', 'rs', 'ch', 'rs', 'rs', 'ib', 'rs', 'ib', 'ch', 'rs', 'ib', 'ch', 'ch', 'rs', 'ch', 'ch', 'ib', 'ch', 'rs', 'ch', 'ib', 'rs', 'ib', 'ib', 'ib', 'ch', 'rs', 'ib', 'rs', 'ch', 'ch', 'ib', 'rs', 'rs', 'ib', 'rs', 'rs', 'rs', 'ch', 'ch', 'ch', 'ib', 'rs', 'ch', 'rs', 'rs', 'ch', 'rs', 'rs', 'ib', 'rs', 'ch', 'ib', 'rs', 'ib', 'ib', 'rs', 'ch', 'ib', 'rs', 'rs', 'rs', 'rs', 'ib', 'ib', 'ib', 'ib', 'ch', 'ib', 'rs', 'rs', 'ch', 'ib', 'ib', 'ch', 'ch', 'ib', 'ib', 'ib', 'ch', 'rs', 'ch', 'ch', 'ch', 'ch', 'ib', 'ch', 'ch', 'ch', 'ib', 'rs', 'ch', 'ib', 'rs', 'ib', 'ch', 'ch', 'rs', 'ib', 'rs', 'ib', 'ch', 'ib', 'rs', 'ib', 'ch', 'rs', 'ch', 'ib', 'ch', 'ch', 'ib', 'ib', 'ib', 'rs', 'ib', 'rs', 'ch', 'ib', 'ib', 'rs', 'rs', 'ib', 'ib', 'rs', 'ib', 'ib', 'ib', 'ib', 'ib', 'ch', 'ib', 'ib', 'rs', 'ch', 'ib', 'ib', 'ib', 'ib', 'ch', 'ch', 'ch', 'ib', 'ch', 'ib', 'rs', 'rs', 'ib', 'rs', 'rs', 'ib', 'ch', 'ch', 'ch', 'ch', 'ch', 'rs', 'ch', 'ch', 'ch', 'rs', 'ib', 'ch', 'rs', 'ib', 'rs', 'rs', 'ib', 'ch', 'ch', 'rs', 'ch', 'ch', 'rs', 'ib', 'rs', 'ib', 'ch', 'rs', 'ib', 'ch', 'rs', 'rs', 'ch', 'rs', 'ib', 'rs', 'rs', 'ch', 'ib', 'ch', 'ch', 'ch', 'ib', 'rs', 'ch', 'ib', 'ch', 'ib', 'ch', 'rs', 'ch', 'ch', 'rs', 'ib', 'ch', 'ib', 'ch', 'ib', 'ib', 'rs', 'ch', 'ib', 'rs', 'ch', 'ch', 'ch', 'rs', 'ib', 'rs', 'ib', 'ch', 'ib', 'ch', 'ch', 'ch', 'rs', 'ib', 'rs', 'rs', 'ib', 'ib', 'ib', 'ib', 'ch', 'rs', 'rs', 'ch', 'ch', 'ch', 'ch', 'ch', 'rs', 'ib', 'rs', 'rs', 'ch', 'rs', 'rs', 'ch', 'ib', 'ch', 'ch', 'rs', 'rs', 'rs', 'rs', 'ch', 'ib', 'ch', 'ch', 'ch', 'rs', 'rs', 'ch', 'ch', 'ch', 'rs', 'ib', 'ch', 'ch', 'ib', 'ch', 'ch', 'rs', 'ib', 'rs', 'rs', 'rs', 'rs', 'ib', 'ib', 'rs', 'ch', 'ib', 'ib', 'ch', 'rs', 'ib', 'rs', 'ib', 'rs', 'ch', 'rs', 'rs', 'ib', 'ch', 'rs', 'ch', 'rs', 'ib', 'ch', 'ib', 'ch', 'ch', 'ib', 'rs', 'ib', 'ch', 'ch', 'ib', 'ch', 'rs', 'rs', 'rs', 'ch', 'rs', 'rs', 'ch', 'ib', 'ch', 'ib', 'ch', 'ch', 'rs', 'ch', 'ib', 'ib', 'ch', 'ib', 'rs', 'rs', 'ib', 'rs', 'ch', 'ch', 'rs', 'ch', 'ib', 'ib', 'ib', 'ib', 'rs', 'ib', 'ib', 'rs', 'ib', 'ib', 'ib', 'ch', 'rs', 'ib', 'ib', 'ib', 'ch', 'rs', 'rs', 'rs', 'rs', 'ib', 'ib', 'ch', 'rs', 'ch', 'rs', 'rs', 'rs', 'rs', 'ib', 'ib', 'ib', 'rs', 'ch', 'ib', 'ib', 'ib', 'rs', 'ib', 'ch', 'rs', 'ib', 'rs', 'ib', 'ch', 'ib', 'ch', 'rs', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'rs', 'ib', 'ib', 'ib', 'ib', 'ch', 'rs', 'ch', 'rs', 'ch', 'rs', 'ch', 'rs', 'ib', 'ch', 'rs', 'rs', 'rs', 'rs', 'ib', 'rs', 'ch', 'rs', 'rs', 'rs', 'rs', 'ch', 'rs', 'rs', 'ch', 'ch', 'ch', 'ib', 'rs', 'ib', 'ch', 'rs', 'ib', 'ch', 'ib', 'ib', 'ib', 'ch', 'ib', 'ib', 'ib', 'ch', 'rs', 'ib', 'rs', 'ib', 'ib', 'rs', 'ib', 'ch', 'ch', 'ch', 'rs', 'ch', 'rs', 'ch', 'rs', 'rs', 'ch', 'rs', 'ib', 'ch', 'rs', 'ib', 'ib', 'ib', 'ch', 'ch', 'ib', 'rs', 'ib', 'ch', 'rs', 'ib', 'ch', 'ch', 'ch', 'ch', 'rs', 'ch', 'ch', 'ib', 'rs', 'rs', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ib', 'rs', 'ib', 'rs', 'ch', 'ib', 'ch', 'ch', 'ib', 'rs', 'rs', 'ch', 'ch', 'rs', 'rs', 'ib', 'ib', 'rs', 'rs', 'ch', 'rs', 'ib', 'ch', 'ib', 'ib', 'rs', 'rs', 'rs', 'ch', 'ch', 'rs', 'rs', 'ch', 'ib', 'ch', 'ib', 'rs', 'rs', 'rs', 'ch', 'rs', 'ib', 'rs', 'rs', 'ch', 'ib', 'rs', 'ib', 'ch', 'rs', 'rs', 'ib', 'ch', 'ch', 'ch', 'rs', 'rs', 'ch', 'rs', 'ch', 'rs', 'ib', 'ib', 'ch', 'ib', 'rs', 'rs', 'rs', 'ib', 'rs', 'ib', 'rs', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'rs', 'ch', 'ib', 'ch', 'rs', 'ch', 'rs', 'rs', 'ib', 'ib', 'ib', 'ch', 'ch', 'ch', 'ib', 'ib', 'rs', 'ch', 'rs', 'ch', 'rs', 'rs', 'ch', 'rs', 'rs', 'rs', 'ch', 'ib', 'ib', 'ib', 'ib', 'rs', 'ib', 'rs', 'rs', 'ib', 'ib', 'ib', 'ib', 'ib', 'ch', 'rs', 'rs', 'ch', 'ch', 'ch', 'rs', 'rs', 'ch', 'rs', 'ch', 'ib', 'rs', 'ib', 'ib', 'rs', 'rs', 'ch', 'ib', 'ch', 'ib', 'rs', 'rs', 'ib', 'ib', 'ib', 'rs', 'ch', 'ch', 'rs', 'ib', 'ib', 'rs', 'ch', 'ib', 'ib', 'ch', 'ch', 'ib', 'ib', 'ib', 'ib', 'ch', 'rs', 'ib', 'ch', 'rs', 'ib', 'ib', 'rs', 'ch', 'rs', 'ib', 'ch', 'ib', 'ib', 'rs', 'rs', 'rs', 'ib', 'ib', 'ch', 'ib', 'ib', 'ch', 'rs', 'rs', 'rs', 'rs', 'ib', 'ch', 'ib', 'ch', 'rs', 'ib', 'ch', 'ib', 'ch', 'ch', 'ib', 'ch', 'ch', 'ib', 'rs', 'rs', 'ib', 'rs', 'rs', 'rs', 'ch', 'ch', 'ib', 'ch', 'rs', 'rs', 'ch', 'rs', 'ib', 'ib', 'ib', 'ib', 'ib', 'rs', 'ib', 'ch', 'rs', 'rs', 'ib', 'rs', 'ib', 'rs', 'rs', 'rs', 'ib', 'ib', 'rs', 'ch', 'ib', 'ib', 'ib', 'ch', 'rs', 'ib', 'ib', 'ib', 'ib', 'ib', 'ch', 'ch', 'ib', 'rs', 'ch', 'ch', 'rs', 'ch', 'ib', 'ch', 'ib', 'ch', 'rs', 'rs', 'ch', 'ib', 'ib', 'ch', 'ib', 'ch', 'rs', 'ch', 'ch', 'rs', 'rs', 'rs', 'rs', 'rs', 'ch', 'ib', 'rs', 'ib', 'ch', 'rs', 'rs', 'rs', 'ch', 'ib'),)\n"
     ]
    }
   ],
   "source": [
    "import nevergrad as ng\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "parametrization = ng.p.Tuple(\n",
    "    ng.p.Choice(['rs','ch','ib'], repetitions=izh_res.n_neurons)\n",
    ")\n",
    "\n",
    "optimizer = ng.optimizers.NoisyDiscreteOnePlusOne(parametrization=parametrization, budget=10)\n",
    "partial_objective_function = partial(objective_function, res_=izh_res, X_stimuli=X, labels=y)\n",
    "\n",
    "def print_progress(optimizer):\n",
    "    for i in range(optimizer.budget):\n",
    "        x = optimizer.ask()\n",
    "        loss = -partial_objective_function(params=x.value)\n",
    "        optimizer.tell(x, loss)\n",
    "        if (i + 1) % 2 == 0:\n",
    "            print(f\"Iteration {i + 1}/{optimizer.budget}, Current loss: {loss}\")\n",
    "\n",
    "print_progress(optimizer)\n",
    "recommendation = optimizer.provide_recommendation()\n",
    "optimal_parms = recommendation.value\n",
    "\n",
    "print(\"optimal neuron types:\", optimal_parms[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:31<00:00,  3.69it/s]\n"
     ]
    }
   ],
   "source": [
    "optimal_a, optimal_b, optimal_c, optimal_d = [\n",
    "    torch.tensor(list(map(lambda x: neuron_parm_dict[x][i], optimal_parms[0]))) for i in ['a', 'b', 'c', 'd']]\n",
    "izh_res.update_parms(a=optimal_a, b=optimal_b, c=optimal_c, d=optimal_d)\n",
    "opt_spike = izh_res.simulate(X, mem_thr=30, train=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([115, 11000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation Accuracy: 0.8260869565217391\n",
      "[[52 12]\n",
      " [ 8 43]]\n",
      "separation: tensor(0.0114)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from neucube.sampler import TemporalBinning\n",
    "from neucube.utils import SeparationIndex\n",
    "\n",
    "sampler = TemporalBinning(bin_size=10)\n",
    "opt_state_vec = sampler.sample(opt_spike)\n",
    "print(opt_state_vec.shape)\n",
    "\n",
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds)\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for train_index, test_index in tqdm(kf.split(opt_state_vec)):\n",
    "    X_train_fold, X_test_fold = opt_state_vec[train_index], opt_state_vec[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "\n",
    "    svm = SVC(kernel='linear', C=2)  # You can specify different kernels ('linear', 'poly', 'rbf', etc.)\n",
    "    svm.fit(X_train_fold, y_train_fold)\n",
    "    y_pred = svm.predict(X_test_fold)\n",
    "    true_labels.extend(y_test_fold)\n",
    "    predicted_labels.extend(y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"10-Fold Cross-Validation Accuracy:\", accuracy)\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "print(\"separation:\", SeparationIndex(opt_state_vec, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
